{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdd075b-824f-4e18-8489-02d09949ead3",
   "metadata": {},
   "source": [
    "# Creating a volatility signature plot for a highly-liquid asset\n",
    "This notebook aims to create a volatility signature plot for AAPL. A volatility signature plot is a graphical tool used in high-frequency data analysis to assess the impact of sampling frequency on realised volatility estimates.\n",
    "\n",
    "## Intuition and purpose of RV, vs Integrated Variance/Volatility\n",
    "- In quantitative finance, we often model log prices \\$\\displaystyle{p_t=log(P_t)}$ as a stochastic process, for example:\n",
    "\n",
    "$$\n",
    "dp_t = \\mu_t dt + sigma_t dW_t\n",
    "$$\n",
    "\n",
    "- If you integrate the instantaneous variance \\$\\displaystyle{\\sigma_t^2}$ over time, you get:\n",
    "\n",
    "$$\n",
    "\\text{Integrated Variance}_{[0,7]} = \\int_0^T \\sigma_t^2 dt\n",
    "$$\n",
    "\n",
    "- This is the true underlying variance of returns over the period, \\$\\displaystyle{[0,7]}$\n",
    "\n",
    "Since we don't observe \\$\\displaystyle{\\sigma_t}$, we approximate the integrated variance using discrete high-frequency data:\n",
    "\n",
    "$$\n",
    "\\text{Realised Variaiance} = \\sum_{i=1}^n r_i^2\n",
    "$$\n",
    "\n",
    "- As sampling frequency increases, i.e. \\$\\displaystyle{\\Delta t \\rightarrow 0}$, the sum of squared returns converges in probability to integrated variance\n",
    "\n",
    "- However, because of market microstructure noise, which inclues: bid-ask bounce, discreteness of price changes, latency, liquidity issues, etc., these effects introduce spurious high-frequency volatility, inflating squared returns and thus realised variance\n",
    "- Therefore, it is a misrepresentation of true volatility as integrated volatility is a theoretical construct of continuous time finance that represents true underlying variance\n",
    "\n",
    "- High-frequency realised variance overestimates it because:\n",
    "\n",
    "$$\n",
    "\\text{Observed RV} = \\text{True Integrated Variance} + \\text{Microstructure Noise}\n",
    "$$\n",
    "\n",
    "- So when sampling too frequently, you're not just measuring volatility, you're also capturing microstructure noise\n",
    "\n",
    "\n",
    "## How to run this notebook\n",
    "- Set .env variable POLYGON_API_KEY=\"your_API_KEY\", or\n",
    "- Pull AAPL minutely data using the function argument pull_or_pickle=\"pickle\" which unpickles `apple_minute_data.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0aa62-8dfb-470f-b061-1a47fdbeaa91",
   "metadata": {},
   "source": [
    "## Imports - core functions are Pandas, NumPy and Matplotlib. \n",
    "- `dotenv` and `OS` correspond to fetching API key from `.env` variables\n",
    "- `requests` is used for the API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893b591-5f64-4e26-9116-61f7ecce8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fc6b4-ea54-4f7a-a734-d812ab6a9d7c",
   "metadata": {},
   "source": [
    "## Sourcing and processing 1 min data for Apple stock\n",
    "- I will use Polygon with my personal API key, you can generate a free API key on their website\n",
    "- There will also be a pkl file `apple_minute_data.pkl` which you can load instead, by calling the function with the `pull_or_picke=\"pickle\"` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf938248-3802-4b15-8f38-6ebc7dcbf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_aapl_minute(pull_or_pickle=\"pull\"):\n",
    "    if os.path.exists(\"apple_minute_data.pkl\") and pull_or_pickle != \"pull\":\n",
    "        return pd.read_pickle(\"apple_minute_data.pkl\").between_time('09:30','16:00').dropna()\n",
    "\n",
    "    else:\n",
    "        load_dotenv()\n",
    "        api_key = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "        url = \"https://api.polygon.io/v2/aggs/ticker/AAPL/range/1/minute/2025-04-05/2025-05-05\"\n",
    "        params = {\n",
    "            \"adjusted\":\"true\",\n",
    "            \"sort\":\"asc\",\n",
    "            \"limit\":50000,\n",
    "            \"apiKey\":api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(url,params=params)\n",
    "        data = response.json()[\"results\"]\n",
    "        df = pd.DataFrame(data)\n",
    "        df[\"t\"] = pd.to_datetime(df[\"t\"],unit=\"ms\")\n",
    "        df.set_index(\"t\",inplace=True)\n",
    "        df = df.between_time('09:30',\"16:00\")\n",
    "        df = df[[\"c\"]]\n",
    "        df.rename(columns={\"c\":\"Close\"}, inplace=True)\n",
    "        df.to_pickle(\"apple_minute_data.pkl\")\n",
    "        return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45eef46-62cf-4ee6-bc7c-d76ebda2c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pull_aapl_minute(pull_or_pickle=\"pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58ee587-aac0-4086-a330-c527182b6661",
   "metadata": {},
   "source": [
    "## Data Preprocessing, \\$\\displaystyle{R^2}$ calculation, and daily realised volatility computation\n",
    "- Resampling to 1, 5, 15, 60 minute intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a89569-4b8d-48d4-b0c9-436076ef06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_intervals = {1: \"1min\", 5: \"5min\", 15: \"15min\", 30:\"30min\",60: \"1h\",120:\"2h\"}\n",
    "daily_realised_volatility = pd.DataFrame()\n",
    "\n",
    "for interval, resampling_interval in frequency_intervals.items():\n",
    "    # Resample to the desired interval using last()\n",
    "    resampled_series = data[\"Close\"].resample(resampling_interval).last()\n",
    "    # Compute squared log returns, drop NaNs\n",
    "    log_returns = np.log(resampled_series / resampled_series.shift(1))\n",
    "    r2 = (log_returns ** 2).dropna()\n",
    "    # Resample to daily and sum\n",
    "    daily_sum = r2.resample(\"1D\").sum()\n",
    "    # Store in DataFrame with daily index\n",
    "    daily_realised_volatility[str(interval)] = daily_sum\n",
    "\n",
    "# Ensure no NaNs remain (optional: forward-fill or drop)\n",
    "daily_realised_volatility = daily_realised_volatility.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b18273-f992-42e0-8066-748da0783e40",
   "metadata": {},
   "source": [
    "## Calculating annualised volatility\n",
    "- The following formula was used after resampling to daily realised volatility\n",
    "- For simplicity, I used `mean()`, however outliers may skew the results.\n",
    "\n",
    "$$\n",
    "RV_{\\text{annualised}} = 100 \\times \\sqrt{252\\times \\sum R_t^2}\n",
    "$$\n",
    "\n",
    "- The idea is that squared returns provide a proxy for volatility. This approach follows the framework of Anderson et al. (2003), who formalised realised variance as a non-parametric estimator of integrated volatility in high-frequency data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50b116-a1ae-489c-ad8b-ba36d91e0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rvs = daily_realised_volatility.mean()\n",
    "result = mean_rvs.to_dict()\n",
    "result = {int(frequency): mean_rv for frequency, mean_rv in result.items()}\n",
    "mean_annual_vols = {frequency: 100 * np.sqrt(252 * mean_rv) for frequency, mean_rv in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9756a7e-1198-4d9a-8bed-1d2e2ec12088",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(mean_annual_vols.keys())\n",
    "y = pd.Series(mean_annual_vols.values())\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90884ec-299d-4c6a-8479-29215052c75e",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "- Below is the plotted Volatility Signature Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e218a-1cf2-47e4-9eff-4f91aff35fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x,y,marker='o')\n",
    "plt.title(\"Volatility Signature Plot\")\n",
    "plt.xlabel(\"Sampling Interval\")\n",
    "plt.ylabel(\"Mean Annualised Realised Volatility (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae816cd3-1427-44b4-8c7a-40919e70822f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- We can conclude that as the sampling interval increases, realised volatility also does, however this does not necessarily reflected integrated volatility\n",
    "- A trade-off can be seen: using higher frequency data, observed volatility is higher due to noise influence; using lower frequency data, observed volatility is underestimated. It is best to use medium frequency data (5-15 minutes) to have a *true signal*\n",
    "\n",
    "## References\n",
    "- Torben Andersen; Tim Bollerslev; Francis Diebold and Paul Labys, (2003), Modeling and Forecasting Realized Volatility, Econometrica, 71, (2), 579-625"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
